{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended root directory /media/aleksejs/DataHDD/work/codes/comp-neuro/analysis-mesoscopic/pub-2020-exploratory-analysis\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "\n",
    "# Append base directory\n",
    "import os,sys,inspect\n",
    "rootname = \"pub-2020-exploratory-analysis\"\n",
    "thispath = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "rootpath = os.path.join(thispath[:thispath.index(rootname)], rootname)\n",
    "sys.path.append(rootpath)\n",
    "print(\"Appended root directory\", rootpath)\n",
    "\n",
    "from mesostat.utils.qt_helper import gui_fnames, gui_fpath\n",
    "from mesostat.utils.hdf5_io import DataStorage\n",
    "from mesostat.utils.system import getfiles_walk\n",
    "from mesostat.utils.matlab_helper import loadmat, matstruct2dict\n",
    "from mesostat.utils.dictionaries import merge_dicts\n",
    "\n",
    "from lib.sych.data_read import read_neuro_perf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling Data From LVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_all_substring_locations(s, subs):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = s.find(subs, start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(subs) # use start += 1 to find overlapping matches\n",
    "        \n",
    "def find_all_substring_rows(l, subs):\n",
    "    return [i for i in range(len(l)) if subs in l[i]]\n",
    "\n",
    "def get_files_raw_df(fpathData):\n",
    "    fileswalk = getfiles_walk(fpathData, ['.lvm'])\n",
    "\n",
    "    # Convert to pandas\n",
    "    df = pd.DataFrame(fileswalk, columns=['path', 'fname'])\n",
    "\n",
    "    # Drop all LVM files that are not of correct format\n",
    "    df = df[df['fname'].str.contains(\"mvg\")]\n",
    "\n",
    "    df['session'] = [os.path.basename(path) for path in df['path']]\n",
    "    df['path'] = [os.path.join(path, fname) for path, fname in zip(df['path'], df['fname'])]\n",
    "    df['mousename'] = [os.path.splitext(fname)[0] for fname in df['fname']]\n",
    "\n",
    "    return df.drop('fname', axis=1)\n",
    "\n",
    "def parse_raw_header(headerText):\n",
    "    dataHeader = headerText.replace(\"\\t\", \" \").split('\\n')\n",
    "    dataHeader = [d.strip() for d in dataHeader]\n",
    "    idxsDate = find_all_substring_rows(dataHeader, \"Date \")\n",
    "    idxsTime = find_all_substring_rows(dataHeader, \"Time \")\n",
    "\n",
    "    dateThis = dataHeader[idxsDate[-1]].split(' ')[-1]\n",
    "    timeThis = dataHeader[idxsTime[-1]].split(' ')[-1][:15] # Too many decimal points in seconds bad\n",
    "    return datetime.datetime.strptime(dateThis + ' ' + timeThis, '%Y/%m/%d %H:%M:%S.%f')\n",
    "\n",
    "def parse_raw_main(mainText):\n",
    "    dataMain = mainText.replace(\"\\t\", \" \").split('\\n')[2:]\n",
    "    arrValue = np.array([s.split(' ')[-1] for s in dataMain if len(s) > 0], dtype=float)\n",
    "    \n",
    "    nChannel = 49\n",
    "    nEntry = len(arrValue)\n",
    "    nTimestep = len(arrValue) // 49\n",
    "    nRemainder = len(arrValue) % 49\n",
    "\n",
    "    if nRemainder != 0:\n",
    "        raise IOError(\"Unexpected array length\", nEntry)\n",
    "\n",
    "    return arrValue.reshape((nTimestep, nChannel))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fpathData = gui_fpath(\"Root directory for raw data\", \"./\")\n",
    "fpathData = '/mnt/neurophys-storage2/Sych/Yaro/data_raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFiles = get_files_raw_df(fpathData)\n",
    "dfFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mice = set(dfFiles['mousename'])\n",
    "mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mousename in mice:\n",
    "    ds = DataStorage('raw_'+mousename+'.h5')\n",
    "    rows = dfFiles[dfFiles['mousename'] == mousename]\n",
    "    \n",
    "    for idx, row in rows.iterrows():\n",
    "        print('Processing', mousename, row['session'])\n",
    "        \n",
    "        with open(row['path'], 'r') as f:\n",
    "            data = f.read()\n",
    "\n",
    "            headerEndKey = \"***End_of_Header***\"\n",
    "            splitIdx = list(find_all_substring_locations(data, headerEndKey))[-1] + len(headerEndKey)\n",
    "\n",
    "            dateTimeThis = parse_raw_header(data[:splitIdx])\n",
    "\n",
    "            caIndMat = parse_raw_main(data[splitIdx:])\n",
    "            \n",
    "            attrsDict = {\n",
    "                'mousename': mousename,\n",
    "                'metric': \"raw\",\n",
    "                'target_dim': \"(timesteps, channels)\",\n",
    "                'datetime': dateTimeThis\n",
    "            }\n",
    "\n",
    "            ds.save_data(row['session'], caIndMat, attrsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving all datasets to data group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files_raw_h5_df(fpath):\n",
    "    fileswalk = getfiles_walk(fpath, ['raw', '.h5'])\n",
    "    df = pd.DataFrame(fileswalk, columns=['path', 'fname'])\n",
    "    df['mousename'] = [os.path.splitext(f)[0][4:] for f in df['fname']]\n",
    "    df['path'] = [os.path.join(path, fname) for path, fname in zip(df['path'], df['fname'])]\n",
    "    return df.drop('fname', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fpathDataH5 = gui_fpath(\"Directory for data files\", \"./\")\n",
    "fpathDataH5 = '/media/aleksejs/DataHDD/work/codes/comp-neuro/analysis-mesoscopic/pub-2020-exploratory-analysis/analysis-sych'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRawH5 = get_files_raw_h5_df(fpathDataH5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in dfRawH5.iterrows():\n",
    "    with h5py.File(row['path'], 'a') as h5file:\n",
    "        if 'data' not in h5file.keys():\n",
    "            grp = h5file.create_group(\"data\")\n",
    "        \n",
    "        for key in h5file.keys():\n",
    "            if key != 'data':\n",
    "                print(key)\n",
    "                session = ''.join(list(h5file[key].attrs['name']))\n",
    "                h5file.move(key, 'data/'+session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRawH5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "for idx, row in dfRawH5.iterrows():\n",
    "    with h5py.File(row['path'], 'r') as h5file:\n",
    "        for session in h5file['data'].keys():\n",
    "            print(session, h5file['data'][session].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark Starts and ends of Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, row in dfRawH5.iterrows():\n",
    "    print(row['mousename'])\n",
    "    \n",
    "    with h5py.File(row['path'], 'a') as h5file:\n",
    "        if 'trialStartIdxs' not in h5file.keys():\n",
    "            grp = h5file.create_group('trialStartIdxs')\n",
    "        if 'interTrialStartIdxs' not in h5file.keys():\n",
    "            grp = h5file.create_group('interTrialStartIdxs')\n",
    "            \n",
    "        for session in list(h5file['data'].keys()):\n",
    "            print(session)\n",
    "\n",
    "            traceThis = h5file['data'][session][:, -1]\n",
    "            traceBin = (traceThis > 2).astype(int)\n",
    "            traceDT = traceBin[1:] - traceBin[:-1]\n",
    "\n",
    "            idxTrialStart = np.where(traceDT==1)[0] + 1\n",
    "            idxIntervStart = np.hstack(([0], np.where(traceDT==-1)[0] + 1))\n",
    "\n",
    "            nTrial = len(idxTrialStart)\n",
    "            nInterv = len(idxIntervStart)\n",
    "            \n",
    "            if nTrial == nInterv:\n",
    "                idxIntervStart = np.hstack((idxIntervStart, [len(traceThis)]))\n",
    "                nInterv += 1\n",
    "\n",
    "            tTrial = idxIntervStart[1:] - idxTrialStart\n",
    "\n",
    "            FPS = 20 if np.median(tTrial) < 200 else 40\n",
    "            \n",
    "            h5file['trialStartIdxs'].create_dataset(session, data=idxTrialStart)\n",
    "            h5file['interTrialStartIdxs'].create_dataset(session, data=idxIntervStart)\n",
    "            h5file['data'][session].attrs['FPS'] = FPS\n",
    "            \n",
    "#             print(nTrial, nInterv, FPS)\n",
    "#             print('low', tTrial[tTrial < 8 * FPS] / FPS)\n",
    "#             print('high', tTrial[tTrial > 12 * FPS] / FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending Channel Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files_channel_labels_df(fpath):\n",
    "    fileswalk = getfiles_walk(fpathDataOrig, ['channel_labels.mat'])\n",
    "    df = pd.DataFrame(fileswalk, columns=['path', 'fname'])\n",
    "\n",
    "    df['mousename'] = [os.path.basename(p) for p in df['path']]\n",
    "    df['path'] = [os.path.join(path, fname) for path, fname in zip(df['path'], df['fname'])]\n",
    "    return df.drop('fname', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fpathDataOrig = gui_fpath(\"Directory for original data tree\", \"./\")\n",
    "fpathDataOrig = '/media/aleksejs/DataHDD/work/data/yaro/neuronal/mvg48'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLabels = get_files_channel_labels_df(fpathDataOrig)\n",
    "dfLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in dfLabels.iterrows():\n",
    "    print(row['mousename'])\n",
    "    \n",
    "    M = loadmat(row['path'])\n",
    "    \n",
    "    rowH5 = dfRawH5[dfRawH5['mousename'] == row['mousename']]\n",
    "    pathH5 = list(rowH5['path'])[0]\n",
    "    \n",
    "    with h5py.File(pathH5, 'a') as h5file:\n",
    "        if 'channelLabels' not in h5file.keys():\n",
    "            h5file.create_dataset('channelLabels', data=M['channel_labels'].astype('S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding behaviour\n",
    "\n",
    "**Problems/Bugs**:\n",
    "1. [early_go_trials, iGO_inhibition] overlap - within this framework could be solved by multiplexing enum\n",
    "2. mvg_8_2018_11_22_a has 406 trials in behaviour but only 142 in neuro - crop to neuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files_orig_neuro_df(fpath):\n",
    "    fileswalk = getfiles_walk(fpathDataOrig, ['data.mat'])\n",
    "    df = pd.DataFrame(fileswalk, columns=['path', 'fname'])\n",
    "\n",
    "    df['session'] = [os.path.basename(p) for p in df['path']]\n",
    "    df['mousename'] = [os.path.basename(os.path.dirname(p)) for p in df['path']]\n",
    "    return df.drop('fname', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNeuro = get_files_orig_neuro_df(fpathDataOrig)\n",
    "dfNeuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mvg_4_2017_11_10_a 1000 1 1000 1000 True\n",
      "mvg_4_2017_11_13_a 896 1 896 896 True\n",
      "mvg_4_2017_11_14_a 810 1 810 810 True\n",
      "mvg_4_2017_11_15_a 370 1 370 370 True\n",
      "mvg_4_2017_11_16_a 640 1 640 640 True\n",
      "mvg_4_2017_11_17_a 540 1 540 540 True\n",
      "mvg_4_2017_11_20_a 682 1 682 682 True\n",
      "mvg_4_2017_11_21_a 700 1 700 700 True\n",
      "mvg_4_2017_11_22_a 892 1 892 892 True\n",
      "mvg_4_2017_11_23_a 715 1 715 715 True\n",
      "mvg_4_2017_11_24_a 700 1 700 700 True\n",
      "mvg_8_2018_11_12_a 354 1 354 354 True\n",
      "mvg_8_2018_11_13_a 282 1 282 282 True\n",
      "mvg_8_2018_11_14_a 430 1 430 430 True\n",
      "mvg_8_2018_11_15_a 692 1 692 692 True\n",
      "mvg_8_2018_11_16_a 458 1 458 458 True\n",
      "mvg_8_2018_11_17_a 554 1 554 554 True\n",
      "mvg_8_2018_11_18_a 310 1 310 310 True\n",
      "mvg_8_2018_11_19_a 242 1 242 242 True\n",
      "mvg_8_2018_11_20_a 385 1 385 385 True\n",
      "mvg_8_2018_11_21_a 334 1 334 334 True\n",
      "mvg_8_2018_11_22_a 406 1 406 142 False\n",
      "--cropping to neuronal 142\n",
      "mvg_8_2018_11_23_a 358 1 358 358 True\n",
      "mvg_8_2018_11_27_a 527 1 527 527 True\n",
      "mvg_8_2018_11_28_a 456 1 456 456 True\n",
      "mvg_8_2018_11_29_a 572 1 572 572 True\n",
      "mvg_8_2018_12_04_a 341 1 341 341 True\n",
      "mvg_7_2018_11_09_a 65 1 65 65 True\n",
      "mvg_7_2018_11_12_a 260 1 260 260 True\n",
      "mvg_7_2018_11_13_a 315 1 315 315 True\n",
      "mvg_7_2018_11_14_a 445 1 445 445 True\n",
      "mvg_7_2018_11_15_a 470 1 470 470 True\n",
      "mvg_7_2018_11_17_a 622 1 622 622 True\n",
      "mvg_7_2018_11_18_a 614 1 614 614 True\n",
      "mvg_7_2018_11_19_a 782 1 782 782 True\n",
      "mvg_7_2018_11_20_a 605 1 605 605 True\n",
      "mvg_7_2018_11_21_a 750 1 750 750 True\n",
      "mvg_7_2018_11_22_a 770 1 770 770 True\n",
      "mvg_7_2018_11_23_a 782 1 782 782 True\n",
      "mvg_7_2018_11_24_a 724 1 724 724 True\n",
      "mvg_7_2018_11_26_a 225 1 225 225 True\n",
      "mvg_7_2018_11_28_a 570 1 570 570 True\n",
      "mvg_7_2018_11_29_a 638 1 638 638 True\n",
      "mvg_7_2018_12_04_a 330 1 330 330 True\n",
      "mvg_7_2018_12_05_a 370 1 370 370 True\n",
      "mvg_9_2019_02_06_a 450 1 450 450 True\n",
      "mvg_9_2019_02_07_a 1000 1 1000 1000 True\n",
      "mvg_9_2019_02_08_a 615 1 615 615 True\n",
      "mvg_9_2019_02_12_a 138 1 138 138 True\n",
      "mvg_9_2019_02_13_a 870 1 870 870 True\n",
      "mvg_9_2019_02_14_a 752 1 752 752 True\n",
      "mvg_9_2019_02_18_a 698 1 698 698 True\n",
      "mvg_9_2019_02_19_a 810 1 810 810 True\n",
      "mvg_9_2019_02_20_a 825 1 825 825 True\n",
      "mvg_9_2019_02_21_a 897 1 897 897 True\n",
      "mvg_9_2019_02_23_a 336 1 336 336 True\n",
      "mvg_9_2019_02_24_a 482 1 482 482 True\n",
      "mvg_9_2019_02_25_a 727 1 727 727 True\n",
      "mvg_9_2019_02_26_a 874 1 874 874 True\n",
      "mvg_9_2019_02_27_a 771 1 771 771 True\n",
      "mvg_9_2019_03_04_a 690 1 690 690 True\n",
      "mvg_9_2019_03_05_a 380 1 380 380 True\n",
      "mvg_9_2019_03_06_a 688 1 688 688 True\n",
      "mvg_9_2019_03_07_a 589 1 589 589 True\n",
      "mvg_9_2019_03_08_a 740 1 740 740 True\n",
      "mvg_9_2019_03_09_a 586 1 586 586 True\n"
     ]
    }
   ],
   "source": [
    "keysNeeded = ['iGO', 'iNOGO', 'iFA', 'iMISS']\n",
    "\n",
    "for mousename in set(dfNeuro['mousename']):\n",
    "    rows = dfNeuro[dfNeuro['mousename'] == mousename]\n",
    "    \n",
    "    rowH5 = dfRawH5[dfRawH5['mousename'] == mousename]\n",
    "    pathH5 = list(rowH5['path'])[0]\n",
    "    \n",
    "    with h5py.File(pathH5, 'a') as h5file:\n",
    "        if 'trialTypeNames' not in h5file.keys():\n",
    "            h5file.create_dataset('trialTypeNames', data=np.array(keysNeeded).astype('S'))\n",
    "        if 'trialTypes' not in h5file.keys():\n",
    "            grp = h5file.create_group('trialTypes')\n",
    "        \n",
    "        for idx, row in rows.iterrows():\n",
    "            session = row['session']\n",
    "\n",
    "            pwd = os.path.join(row['path'], 'behaviorvar.mat')\n",
    "\n",
    "            behavior = loadmat(pwd)\n",
    "    #         behavior['trials'] = merge_dicts([matstruct2dict(obj) for obj in behavior['trials']])\n",
    "            fixint = lambda v: v if not isinstance(v, int) else np.array([v])\n",
    "            behavior = {k : fixint(v) for k, v in behavior.items()}\n",
    "    \n",
    "            keysLst = set([key for key in behavior.keys() if len(behavior[key]) > 0])\n",
    "            keysLst -= set(['trials'])\n",
    "            keysLst = list(sorted(keysLst))\n",
    "            \n",
    "#             for keyA in keysLst:\n",
    "#                 for keyB in keysLst:\n",
    "#                     if keyA != keyB:\n",
    "#                         inter = set(behavior[keyA]).intersection(set(behavior[keyB]))\n",
    "#                         if len(inter) > 0:\n",
    "#                             print(keyA, keyB, len(inter))\n",
    "\n",
    "            minTrial = np.min([np.min(behavior[k]) for k in keysLst if len(behavior[k]) > 0])\n",
    "            maxTrial = np.max([np.max(behavior[k]) for k in keysLst if len(behavior[k]) > 0])\n",
    "\n",
    "            nTrialsExp = len(h5file['trialStartIdxs'][session])\n",
    "\n",
    "            enumArr = np.full(maxTrial, -1, dtype=int)\n",
    "            for i, key in enumerate(keysNeeded):\n",
    "                idxs = (behavior[key] - 1).astype(int)\n",
    "                assert np.all(enumArr[idxs] == -1)\n",
    "                enumArr[idxs] = i\n",
    "            \n",
    "            print(session, len(enumArr==-1), minTrial, maxTrial, nTrialsExp, maxTrial==nTrialsExp)\n",
    "            \n",
    "            if maxTrial > nTrialsExp:\n",
    "                print('--cropping to neuronal', nTrialsExp)\n",
    "                enumArr = enumArr[:nTrialsExp]\n",
    "            \n",
    "            del h5file['trialTypes'][session]\n",
    "            \n",
    "            h5file['trialTypes'].create_dataset(session, data=enumArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36qt5)",
   "language": "python",
   "name": "py36qt5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
