{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each mouse\n",
    "1. (Manual) Find summary excel file, extract names of green and yellow sessions\n",
    "2. VidsDCIMG = Find video files, one for each trial\n",
    "3. refImg = Find reference image\n",
    "4. trialTypes = Find trial types\n",
    "5. t1.mat, t2.mat = Find warping files \n",
    "6. allenBrain = Find allen brain map\n",
    "\n",
    "Processing:\n",
    "1. Testing_A:\n",
    "    * For each mouse, load ::: refImg, t1.mat, allenBrain\n",
    "      * Gallero\\SDT-TDT\\overlay\\mou_9\n",
    "        - L_modified (brain areas for all mice) ::: Gallero\\SDT-TDT\\overlay\\mou_9/L_modified.mat\n",
    "        - RefImg ::: Gallero\\SDT-TDT\\overlay\\mou_9\\refImg_ROIs.mat\n",
    "    * For each session, load ::: t2.mat\n",
    "      - Gallero/mou_9/TDT/20180315/widefield_labview/ROI_Allen/registration_transform_1510.mat\n",
    "    * Apply transform to refImg, overplot allenBrain, check that they align\n",
    "2. Testing_B:\n",
    "    * Load all trial videos\n",
    "        - Gallero\\mou_9\\TDT\\20180301\\widefield_labview\n",
    "    * Compute time-average images\n",
    "    * Apply same as Testing_A\n",
    "    * Plot a few random sessions\n",
    "3. Process real data - for each session:\n",
    "    * Provide list of sessions to analyze\n",
    "    * Provide root folder to mouse data\n",
    "    * Provide root folder to overlay data\n",
    "    * Videos\n",
    "        * Load all trial videos\n",
    "        * Apply transformation\n",
    "        * Apply allen map to extract activities, average over pixels\n",
    "        * Store in HDF5\n",
    "    * Trial Intervals:\n",
    "        * Load trial interval file\n",
    "          - Gallero\\mou_9\\TDT\\20180301\\widefield_labview\\a\\Matt_files\\20180301a.mat\n",
    "        * Store 3rd column to HDF5\n",
    "    * Trial Types:\n",
    "        * Load trial type file\n",
    "          - \\Gallero\\mou_9\\TDT\\20180301\\widefield_labview\\a\\Matt_files/trials_ind.mat\n",
    "        * Parse, store to HDF5\n",
    "4. Postprocess datatypes\n",
    "    * Read HDF5\n",
    "    * Merge trials for each session\n",
    "    * Calc times from intervals\n",
    "    * Apply polyfit, look\n",
    "    * Perform DFF, store\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Matlab Fitting routines\n",
    "   - t1= align to mapping day (functional); aligns every day to a common day.\n",
    "   - t2= aligns the common day to Allen institue\n",
    "\n",
    "`\n",
    "tform = fitgeotrans(movingPoints,fixedPoints,'polynomial',3);\n",
    "Jregistered = imwarp(refImg*7000,tform,'OutputView',imref2d(size(A_scaled))); %aligned map\n",
    "`\n",
    "\n",
    "**TODO**:\n",
    "* Compare TrialTypes to to trial_idx.m where applicable, otherwise trial_error.m\n",
    "* Compare resulting bn_trial with what Yasir got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended root directory /home/alyosha/work/git/pub-2020-exploratory-analysis\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "# Append base directory\n",
    "import os,sys,inspect\n",
    "rootname = \"pub-2020-exploratory-analysis\"\n",
    "thispath = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "rootpath = os.path.join(thispath[:thispath.index(rootname)], rootname)\n",
    "sys.path.append(rootpath)\n",
    "print(\"Appended root directory\", rootpath)\n",
    "\n",
    "from mesostat.utils.qt_helper import gui_fpath\n",
    "\n",
    "from lib.gallerosalas.preprocess_raw import preprocess\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 20  # Hz\n",
    "mice = ['mou_9', 'mou_6', 'mou_7', 'mou_5']\n",
    "extra_mice = ['mou_15_Ariel', 'mou_18_Ariel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pathTGT = gui_fpath('root path to TGT')\n",
    "pathTGT = '/run/user/1000/gvfs/smb-share:server=130.60.51.15,share=neurophysiology-storage2/Gallero'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pathOverlay = gui_fpath('root path to overlay')\n",
    "pathOverlay = '/run/user/1000/gvfs/smb-share:server=130.60.51.15,share=neurophysiology-storage2/Gallero/SDT-TDT/overlay'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathPreferences = gui_fpath('root path to overlay')\n",
    "pathPreferences = '/media/alyosha/Data/TE_data/yasirdata_raw'\n",
    "# pathPreferences = '/media/aleksejs/DataHDD/work/data/yasir/yasirdata_raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathDict = {\n",
    "    'TGT' : pathTGT,\n",
    "    'Overlay' : pathOverlay,\n",
    "    'Preferences' : pathPreferences\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prep = preprocess(pathDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Test transform on Ref files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mouseName, pathRef in prep.pathRef.items():\n",
    "    img = prep.load_ref_img(pathRef)\n",
    "    t2 = prep.load_t2(prep.pathT2[mouseName])\n",
    "    print(mouseName)\n",
    "    \n",
    "    imgT2 = prep.transform_img(img, t2)[0]\n",
    "    prep.plot_transforms(img, imgT2, imgT1=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Test transform on Video Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for mousename in sorted(set(prep.dataPaths['mouse'])):\n",
    "    print(mousename)\n",
    "    prep.test_transform_vids(mousename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pool-process video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep.process_video_files('mou_5', skipExisting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Test if result is sensible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "rez = []\n",
    "for idx, row in prep.dataPaths.iterrows():\n",
    "    session = row['day'] + '_' + row['session']\n",
    "    fpath = os.path.join(pathPreferences, row['mouse'] + '.h5')    \n",
    "    \n",
    "    with h5py.File(fpath, 'r') as f:\n",
    "        dataRSP = np.copy(f['data'][session])\n",
    "        rez += [np.nanmean(dataRSP, axis=(0, 1))]\n",
    "        \n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(pd.DataFrame(np.array(rez), columns=np.arange(27)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Pull trial types and times\n",
    "\n",
    "* Also compute d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "for mouseName in mice:\n",
    "    with h5py.File(os.path.join(pathPreferences, mouseName + '.h5'), 'a') as f:\n",
    "        del f['metadata']\n",
    "        del f['accuracy']\n",
    "        del f['dprime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep.process_metadata_files(pathPreferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in prep.dataPaths.iterrows():\n",
    "    session = row['day'] + '_' + row['session']\n",
    "    fpath = os.path.join(pathPreferences, row['mouse'] + '.h5')\n",
    "    df = pd.read_hdf(fpath, '/metadata/' + session)\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Baseline subtraction\n",
    "\n",
    "# 6.1 Plot stitched session, fit poly\n",
    "\n",
    "**TODO**:\n",
    "* Purge mismatching trial counts for trialStruct vs vid:\n",
    "    - Go over data, delete last\n",
    "* Very high variance of individual consecutive trials. May render entire idea useless\n",
    "  - Explore more channels, mice, naive vs expert\n",
    "  - Compare if Yaro has the same problem\n",
    "  - Ask Yasir what he thinks about the origin of these fluctuations:\n",
    "      - Real?\n",
    "      - Motion artifacts?\n",
    "      - Sth else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "for idx, row in prep.dataPaths.iterrows():\n",
    "    try: \n",
    "#         session = row['day'] + '_' + row['session']\n",
    "        session=\"2017_03_29_session01\"\n",
    "        print(row['mouse'], session)\n",
    "        prep.example_poly_fit(pathPreferences, row['mouse'], session, ord=50, iCh=6)\n",
    "    except:\n",
    "        print('-- Failed')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Process all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FPS = 20\n",
    "iMin = 0 * FPS\n",
    "iMax = 1 * FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prep.crop_mismatch_trials(pathPreferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep.baseline_subtraction_dff(pathPreferences, iMin, iMax, skipExist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep.baseline_subtraction_poly(pathPreferences, ord=50, skipExist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Purification\n",
    "\n",
    "### 7.1 Drop bad sessions\n",
    "\n",
    "- mou_7 20171129_c\n",
    "- mou_6 20171006_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.drop_preprocess_session(pathPreferences, 'mou_6', '20171006_a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.drop_preprocess_session(pathPreferences, 'mou_7', '20171129_c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Add another performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180301_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180301_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180302_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180302_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20180302_c\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180305_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180305_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180306_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180306_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20180307_a\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20180307_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20180307_c\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180308_a\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20180308_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20180309_a\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20180309_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180313_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180313_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180313_c\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180315_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20180316_a\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20180316_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20170925_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20170925_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20170925_c\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20170926_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20170926_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20170926_c\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20170927_a\n",
      "{'Hit', 'CR', 'Early', 'Miss'}\n",
      "20170927_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20170927_c\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20170928_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20170928_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20170929_a\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20170929_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20170929_c\n",
      "{'Hit', 'CR', 'Early', 'Miss'}\n",
      "20171002_a\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20171002_c\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20171004_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171005_a\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20171005_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171006_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20171009_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171009_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20171010_a\n",
      "{'Hit', 'CR', 'Early', 'FA'}\n",
      "20171010_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20171010_c\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20171011_a\n",
      "{'Hit', 'CR', 'Early', 'Miss'}\n",
      "20171011_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171011_c\n",
      "{'Hit', 'CR', 'Early', 'Miss'}\n",
      "20171012_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171012_c\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171113_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171114_a\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20171114_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171114_c\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171115_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171115_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20171115_c\n",
      "{'Hit', 'CR', 'Early', 'FA'}\n",
      "20171115_d\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171116_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20171122_a\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20171122_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171122_c\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171122_d\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171123_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171123_b\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "20171127_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171127_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171127_c\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171128_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171128_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171129_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171129_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171129_d\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171129_e\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171130_a\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171130_b\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "20171130_c\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "2017_03_06_session01\n",
      "{'Hit', 'CR', 'Early', 'FA'}\n",
      "2017_03_06_session02\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "2017_03_13_session01\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "2017_03_13_session02\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "2017_03_14_session02\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "2017_03_15_session01\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "2017_03_16_session01\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "2017_03_16_session02\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "2017_03_16_session03\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "2017_03_17_session01\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "2017_03_22_session01\n",
      "{'Hit', 'CR', 'Early', 'FA'}\n",
      "2017_03_22_session03\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "2017_03_22_session04\n",
      "{'Hit', 'CR', 'Early', 'Miss'}\n",
      "2017_03_23_session01\n",
      "{'Hit', 'CR', 'Early', 'FA'}\n",
      "2017_03_23_session02\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "2017_03_24_session01\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "2017_03_24_session02\n",
      "{'Early', 'FA', 'Miss', 'Hit', 'CR'}\n",
      "2017_03_24_session03\n",
      "{'Hit', 'CR', 'Early', 'Miss'}\n",
      "2017_03_28_session01\n",
      "{'Hit', 'CR', 'Miss', 'FA'}\n",
      "2017_03_29_session01\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n",
      "2017_03_29_session02\n",
      "{'Early', 'Miss', 'FA', 'Hit', 'CR'}\n"
     ]
    }
   ],
   "source": [
    "for mousename in mice:\n",
    "    fpath = os.path.join(pathPreferences, mousename + '.h5')\n",
    "    with h5py.File(fpath, 'r') as h5file:\n",
    "        sessions = list(h5file['metadata'].keys())\n",
    "        \n",
    "    for session in sessions:\n",
    "        print(session)\n",
    "        df = pd.read_hdf(fpath, '/metadata/'+session)\n",
    "        print(set(df['trialType']))\n",
    "        \n",
    "#         display(df)\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3ml",
   "language": "python",
   "name": "py3ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
